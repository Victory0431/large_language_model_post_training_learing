# 大模型后训练（Post-training）核心学习大纲
## 一、基础核心：Transformer 与 LLM 基础（理论 + 入门实操）
1. Transformer 核心原理（理论）
1.1 编码器（Encoder）结构：多头注意力、Feed-Forward 网络、LayerNorm
1.2 解码器（Decoder）结构：掩码多头注意力、编码器-解码器注意力
1.3 关键机制：位置编码（绝对/相对）、残差连接、缩放点积注意力
1.4 Transformer 的并行性优势（与 RNN 对比）
2. 大语言模型（LLM）基础（理论 + 实操）
2.1 预训练目标：自回归语言建模（Causal LM）、掩码语言建模（MLM）（理论）
2.2 主流 LLM 架构差异：Llama 系列、Qwen 系列、ChatGLM 系列（理论）
2.3 Hugging Face 生态使用（实操）：
2.3.1 模型加载（AutoModelForCausalLM）、Tokenizer 使用（含特殊token处理）
2.3.2 基础推理（生成文本、对话交互）、格式封装
2.3.3 模型保存、权重转换与框架适配
2.4 大模型显存优化基础：混合精度训练（FP16/FP8）、梯度检查点、模型并行（理论+实操）

## 二、预训练与后训练衔接（理论为主）
1. 预训练核心逻辑（聚焦后训练关联）
1.1 预训练数据质量与领域适配对后训练的影响
1.2 预训练 checkpoint 选择与后训练初始化
1.3 预训练评估指标（Perplexity）与后训练启动条件
2. 工业级预训练关键技术（后训练分布式基础）
2.1 分布式框架核心概念：DeepSpeed、Megatron-LM
2.2 并行策略：数据并行（DP）、模型并行（TP）、混合并行（DP+TP）
2.3 预训练效率优化：梯度累积、混合精度、checkpoint 合并

## 三、第一阶段：有监督微调（SFT）（理论 + 核心实操）
1. SFT 核心理论
1.1 SFT 定义与价值：任务对齐目标
1.2 SFT 与预训练的区别：目标、数据、训练强度
1.3 SFT 关键参数：学习率、批次大小、训练轮数、权重衰减
1.4 轻量化微调技术（核心）：
1.4.1 LoRA 原理与参数配置（r、alpha、target_modules）
1.4.2 QLoRA（4bit/8bit 量化）原理与显存优化效果
1.4.3 全参数微调 vs 轻量化微调：适用场景与优缺点
2. SFT 实操（以 LLaMA-Factory 为主）
2.1 数据集准备：
2.1.1 数据格式：单轮（Alpaca 格式）、多轮（ShareGPT 格式）
2.1.2 数据集处理：清洗、去重、脱敏、格式转换
2.1.3 自定义数据集适配（行业专属数据）
2.2 训练执行：
2.2.1 配置文件编写（模型、数据、训练参数）
2.2.2 单 GPU/多 GPU 训练脚本执行
2.2.3 训练监控：Loss 曲线、显存占用、生成样例
2.3 模型评估与导出：
2.3.1 自动评估：困惑度、BLEU、准确率
2.3.2 人工评估：指令遵循度、任务适配性
2.3.3 模型合并（LoRA+基础模型）与格式导出
2.4 verl SFT 补充实操（可选）：
2.4.1 FSDP 分布式 SFT 训练
2.4.2 Megatron-LM 后端适配

## 四、第二阶段：直接偏好优化（DPO）（理论 + 实操）
1. DPO 核心理论
1.1 DPO 定义与优势：无需独立奖励模型（RM）
1.2 DPO 与 RLHF 的区别：步骤、数据、效率、显存需求
1.3 核心参数：温度参数（β）、参考模型（ref_model）、KL 约束
2. DPO 实操
2.1 偏好数据集准备：
2.1.1 数据格式：prompt + chosen（优质回答） + rejected（劣质回答）
2.1.2 数据构造：人工标注、模型辅助生成
2.2 训练执行：
2.2.1 TRL 库快速入门训练
2.2.2 LLaMA-Factory 配置文件编写与训练
2.3 评估与调优：
2.3.1 关键指标：DPO Loss、偏好对齐率
2.3.2 参数调优：β 调整、参考模型选择

## 五、第三阶段：强化学习（RL）与人类反馈强化学习（RLHF）（理论 + 进阶实操）
1. RL 基础理论
1.1 核心概念：智能体（Agent）、环境、状态、动作、奖励、策略
1.2 马尔可夫决策过程（MDP）基础
1.3 PPO 核心原理：优势函数、裁剪目标、KL 散度惩罚
2. RLHF 全流程理论
2.1 RLHF 三大步骤：SFT→奖励模型（RM）训练→RL 微调
2.2 奖励模型（RM）：作用、训练数据、对比损失函数
2.3 核心组件：Actor 模型、Critic 模型、参考模型
3. RLHF 实操
3.1 奖励模型（RM）训练：
3.1.1 LLaMA-Factory RM 训练（UltraFeedback 数据集）
3.1.2 自定义奖励函数（规则型/模型型）
3.2 PPO 微调：
3.2.1 LLaMA-Factory 配置文件编写（关联 SFT/RM 模型）
3.2.2 训练流程执行与监控（Policy Loss、Value Loss、奖励均值）
3.3 verl RL 深化实操（核心进阶）：
3.3.1 分布式 PPO 训练（FSDP+vLLM 加速）
3.3.2 高级 RL 算法：GRPO、DAPO
3.3.3 多轮对话 RL 训练
3.4 效果对比：SFT 模型 vs PPO 模型、不同 RL 算法差异

## 六、奖励微调（RFT）体系与算法选型（理论）
1. RFT 定义与分类：显式奖励（PPO/GRPO）、隐式奖励（DPO/DAPO）
2. 主流 RFT 算法对比：显存需求、训练效率、对齐精度、适用场景
3. 算法选型原则：算力匹配、场景需求（准确性/迭代速度）

## 七、后训练数据工程（理论 + 实操）
1. 数据来源：开源数据集、企业私有数据、合成数据
2. 数据质量把控：准确性、多样性、无噪声、格式一致性
3. 数据处理：清洗、去重、脱敏、标注、格式转换
4. 数据增强：同义替换、指令泛化、回译、多轮扩展

## 八、后训练模型评估与迭代（理论 + 实操）
1. 客观评估指标：困惑度、BERTScore、Distinct-n、Self-BLEU、任务准确率
2. 主观评估：评估维度（指令遵循度、准确性、流畅度）、评分表设计、标注流程
3. 迭代策略：数据迭代、参数迭代、算法迭代

## 九、工程化落地与问题排查（理论 + 实操）
1. 模型轻量化：量化（INT4/INT8）、知识蒸馏
2. 部署适配：vLLM 推理加速、Triton 部署、Gradio Demo 搭建
3. 常见问题解决：
3.1 显存不足：量化、LoRA、梯度检查点、BS 调整
3.2 Loss 异常：学习率调整、数据清洗、参数正则化
3.3 生成质量问题：重复度高、偏离任务、对齐效果差

## 十、进阶特性（加分项）
1. 多模态后训练（理论 + 实操）
1.1 多模态 LLM 基础：图文对齐原理
1.2 多模态 SFT：Qwen-VL 微调（LLaMA-Factory）
1.3 多模态 RL/DP：verl 多模态 GRPO/DPO 训练
2. 工具调用后训练（理论 + 实操）
2.1 工具调用原理：函数格式对齐、参数解析
2.2 工具调用 SFT：基于 glaive_toolcall 数据集微调
2.3 工具调用 RL 优化：verl 工具集成训练
3. 分布式训练与性能调优（理论 + 实操）
3.1 FSDP 核心配置（verl 文档）
3.2 推理加速：vLLM/SGLang 集成
3.3 多节点训练配置（可选）

## 十一、项目实战与面试准备
1. 项目实战：
1.1 基础项目：电商客服 SFT+DPO 全流程
1.2 进阶项目：医疗问答 RLHF 全流程
1.3 加分项目：多模态电商客服后训练
2. 项目包装：GitHub 仓库结构、README 编写、效果展示
3. 面试准备：
3.1 高频知识点背诵（Transformer、SFT、DPO/RLHF、LoRA）
3.2 问题排查与实操经验总结
3.3 项目话术（STAR 结构）

要不要我帮你整理一份**各阶段核心实操脚本模板**，包含 SFT、DPO、RLHF 关键步骤的极简代码，方便你快速上手？

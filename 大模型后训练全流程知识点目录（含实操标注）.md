# 大模型后训练全流程知识点目录（含实操标注）

### 一、基础核心：Transformer 与 LLM 基础（理论 + 入门实操）

#### 1. Transformer 核心原理（理论）



* 1.1 编码器（Encoder）结构：多头注意力、Feed-Forward 网络、LayerNorm

* 1.2 解码器（Decoder）结构：掩码多头注意力、编码器 - 解码器注意力

* 1.3 关键机制：位置编码（绝对 / 相对）、残差连接、缩放点积注意力

* 1.4 Transformer 的并行性优势（与 RNN 对比）

#### 2. 大语言模型（LLM）基础（理论 + 实操）



* 2.1 预训练目标：自回归语言建模（Causal LM）、掩码语言建模（MLM）（理论）

* 2.2 主流 LLM 架构差异：Llama 系列、Qwen 系列、ChatGLM 系列（理论）

* 2.3 Hugging Face 生态使用（实操）：


  * 2.3.1 模型加载（`AutoModelForCausalLM`）、Tokenizer 使用

  * 2.3.2 基础推理（生成文本、对话交互）

  * 2.3.3 模型保存与权重转换（适配不同框架）

* 2.4 大模型显存优化基础：混合精度训练（FP16/FP8）、模型并行（理论）

### 二、第一阶段：预训练（Pre-training）（理论为主，了解核心逻辑）

#### 1. 预训练核心逻辑（理论）



* 1.1 预训练数据：无标注文本数据的筛选、清洗、分词

* 1.2 预训练流程：数据加载、训练循环、 checkpoint 保存与加载

* 1.3 预训练评估：困惑度（Perplexity）指标解读

#### 2. 工业级预训练关键技术（理论）



* 2.1 分布式预训练框架：Megatron-LM、DeepSpeed（核心概念）

* 2.2 并行策略：数据并行（DP）、模型并行（TP）、序列并行（SP）

* 2.3 预训练效率优化：梯度累积、混合精度、 checkpoint 合并

### 三、第二阶段：有监督微调（SFT）（理论 + 核心实操）

#### 1. SFT 核心理论（理论）



* 1.1 SFT 定义：用有标注任务数据对齐模型输出与人类需求

* 1.2 SFT 与预训练的区别：目标、数据、训练强度差异

* 1.3 SFT 关键参数：学习率（LR）、批次大小（Batch Size）、训练轮数（Epoch）、权重衰减（Weight Decay）

* 1.4 轻量化微调技术（核心）：


  * 1.4.1 LoRA（Low-Rank Adaptation）原理与参数配置（r、alpha、target\_modules）

  * 1.4.2 QLoRA（量化 + LoRA）：4bit/8bit 量化原理与显存节省效果

  * 1.4.3 全参数微调 vs 轻量化微调：适用场景与优缺点

#### 2. SFT 实操（核心实操，用 LLaMA-Factory 为主）



* 2.1 数据集准备：


  * 2.1.1 数据格式：单轮对话（prompt+response）、多轮对话（ShareGPT 格式）

  * 2.1.2 数据集处理：清洗、去重、格式转换（JSON/Parquet）

  * 2.1.3 自定义数据集适配（如医疗问答、代码生成数据）

* 2.2 LLaMA-Factory SFT 训练：


  * 2.2.1 配置文件编写（模型、数据、训练参数）

  * 2.2.2 单 GPU / 多 GPU 训练脚本执行（`llamafactory-cli train`）

  * 2.2.3 训练过程监控：Loss 曲线、学习率调度、显存占用

* 2.3 模型评估与导出：


  * 2.3.1 自动评估：困惑度、BLEU、准确率（用 LLaMA-Factory 内置指标）

  * 2.3.2 人工评估：对话流畅度、任务适配性（对比微调前后）

  * 2.3.3 模型合并（LoRA 权重 + 基础模型）与导出（`llamafactory-cli export`）

* 2.4 verl SFT 补充实操（可选）：


  * 2.4.1 FSDP 分布式 SFT 训练（`fsdp_sft_trainer.py`）

  * 2.4.2 Megatron-LM 后端适配（大模型场景）

### 四、第三阶段：强化学习（RL）与人类反馈强化学习（RLHF）（理论 + 进阶实操）

#### 1. 强化学习（RL）基础理论（理论）



* 1.1 核心概念：智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）、策略（Policy）

* 1.2 马尔可夫决策过程（MDP）基础

* 1.3 策略优化算法：PPO（Proximal Policy Optimization）核心原理


  * 1.3.1 优势函数（Advantage Function）

  * 1.3.2 裁剪目标（Clipped Objective）

  * 1.3.3 KL 散度惩罚（避免策略更新幅度过大）

#### 2. RLHF 全流程理论（理论）



* 2.1 RLHF 三大步骤：SFT→奖励模型（RM）训练→RL 微调

* 2.2 奖励模型（RM）：


  * 2.2.1 作用：将人类偏好（好 / 坏）量化为数值奖励

  * 2.2.2 训练数据：偏好数据（pairwise 数据，A/B 样本对比）

  * 2.2.3 损失函数：对比损失（Margin Ranking Loss）

* 2.3 策略模型（Actor）与价值模型（Critic）：


  * 2.3.1 Actor：基于 SFT 模型初始化，生成优化后的输出

  * 2.3.2 Critic：预测动作价值，辅助 Actor 更新

  * 2.3.3 参考模型（Reference Model）：稳定训练，计算 KL 散度

#### 3. RLHF 实操（LLaMA-Factory 入门，verl 深化）



* 3.1 奖励模型（RM）训练（实操）：


  * 3.1.1 LLaMA-Factory RM 训练：用 UltraFeedback 数据集训练（`examples/train_rm/`）

  * 3.1.2 自定义奖励函数（规则型：如数学题正确率打分）

* 3.2 LLaMA-Factory PPO 微调（入门实操）：


  * 3.2.1 配置文件编写（关联 SFT 模型、RM 模型）

  * 3.2.2 PPO 训练流程执行（`examples/train_ppo/`）

  * 3.2.3 关键指标监控：Policy Loss、Value Loss、Reward 均值

* 3.3 verl RL 深化实操（核心进阶）：


  * 3.3.1 分布式 PPO 训练（FSDP+vLLM 推理加速）

  * 3.3.2 高级 RL 算法：GRPO（Gradient Regularized PPO）、DAPO（Direct Preference Optimization with Advantage）

  * 3.3.3 多轮对话 RL 训练（`examples/sglang_multiturn/`）

  * 3.3.4 奖励函数定制（`verl/data/``reward_function.py`）

* 3.4 效果对比（实操）：


  * 3.4.1 SFT 模型 vs PPO 模型输出差异（任务准确率、偏好对齐度）

  * 3.4.2 不同 RL 算法（PPO/GRPO）效果对比

### 五、进阶特性（加分项，适配实习竞争力）

#### 1. 多模态后训练（理论 + 实操）



* 1.1 多模态 LLM 基础：图文对齐原理（如 Qwen2.5-VL）（理论）

* 1.2 多模态 SFT：LLaMA-Factory 微调 Qwen2.5-VL（`examples/train_lora/qwen2_vl_lora_sft.yaml`）（实操）

* 1.3 多模态 RL：verl 跑通多模态 GRPO 训练（`examples/grpo_trainer/``run_qwen2_5_vl-7b.sh`）（实操）

#### 2. 工具调用后训练（理论 + 实操）



* 2.1 工具调用原理：函数调用格式对齐、参数解析（理论）

* 2.2 工具调用 SFT：LLaMA-Factory 基于 glaive\_toolcall 数据集微调（`examples/train_lora/llama3_lora_toolcall.yaml`）（实操）

* 2.3 工具调用 RL 优化：verl 工具集成训练（`recipe/retool`）（实操）

#### 3. 分布式训练与性能调优（理论 + 实操）



* 3.1 分布式训练框架：FSDP（完全分片数据并行）核心配置（verl 文档）（理论 + 实操）

* 3.2 推理加速：vLLM/SGLang 集成（verl `rollout`模块）（实操）

* 3.3 显存优化：LoRA + 混合精度 + 梯度检查点（理论 + 实操）

* 3.4 多节点训练：verl 多节点配置（`verl/utils/``launch.py`）（实操，可选）

### 六、项目包装与面试必备（实操 + 背诵）

#### 1. 项目包装（实操）



* 1.1 项目结构：代码（训练脚本 + 配置文件）、文档（README + 效果对比）、模型（导出文件）

* 1.2 项目案例：


  * 1.2.1 项目 1：基于 LLaMA-Factory 的多轮对话 SFT（医疗 / 代码领域）

  * 1.2.2 项目 2：基于 LLaMA-Factory 的 RLHF 全流程（SFT→RM→PPO）

  * 1.2.3 项目 3：基于 verl 的分布式 GRPO 训练（大模型优化）

* 1.3 GitHub 仓库整理：清晰的目录、详细的运行步骤、效果截图

#### 2. 面试高频知识点（背诵）



* 2.1 Transformer：多头注意力的作用、位置编码的必要性

* 2.2 SFT：LoRA 的原理、关键参数影响、轻量化微调的优势

* 2.3 RLHF：三大步骤的作用、PPO 的核心逻辑、KL 散度的作用

* 2.4 框架差异：LLaMA-Factory 与 verl 的适用场景、分布式训练的实现逻辑

* 2.5 问题排查：训练时显存不足的解决方法、Loss 不下降的原因

> （注：文档部分内容可能由 AI 生成）